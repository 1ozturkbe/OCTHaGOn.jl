{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../load.jl\")\n",
    "using Plots\n",
    "using StatsBase\n",
    "function refine_thresholds(gm::GlobalModel, bbr::BlackBoxRegressor)\n",
    "    if length(bbr.active_trees) == 1\n",
    "        best_lower = getvalue(bbr.dependent_var)\n",
    "        best_upper = bbr(solution(gm))[1]\n",
    "        learn_constraint!(bbr, threshold = \"upper\" => best_upper)\n",
    "        update_tree_constraints!(gm, bbr)\n",
    "        learn_constraint!(bbr, threshold = \"lower\" =>best_lower)\n",
    "        update_tree_constraints!(gm, bbr)\n",
    "        return\n",
    "    elseif length(bbr.active_trees) == 2\n",
    "        bds = Dict(collect(values(bbr.active_trees))) # TODO: have a cleaner system for this.\n",
    "        old_lower = bds[\"lower\"]\n",
    "        old_upper = bds[\"upper\"]\n",
    "        new_lower = getvalue(bbr.dependent_var)\n",
    "        new_upper = bbr(solution(gm))[1]\n",
    "        # Updating upper bounds\n",
    "        if new_upper <= old_upper\n",
    "            learn_constraint!(bbr, threshold = \"upper\" => new_upper)\n",
    "            update_tree_constraints!(gm, bbr)\n",
    "        else\n",
    "            learn_constraint!(bbr, threshold = \"upper\" => old_upper) #TODO add warmstarts here. \n",
    "            update_tree_constraints!(gm, bbr)\n",
    "        end\n",
    "        # Updating lower bounds\n",
    "        learn_constraint!(bbr, threshold = \"lower\" => (maximum([old_lower, new_lower]) + minimum([new_upper, old_upper]))/2)\n",
    "        update_tree_constraints!(gm, bbr)\n",
    "        return\n",
    "        # if new_lower >= old_lower\n",
    "        #     learn_constraint!(bbr, threshold = \"lower\" => (new_lower + minimum([new_upper, old_upper])/2)\n",
    "        #     update_tree_constraints!(gm, bbr)\n",
    "        #     return \n",
    "        # else\n",
    "        #     learn_constraint!(bbr, # binary reduce the lower bound\n",
    "        #         threshold = \"lower\" => (new_lower + old_lower)/2)\n",
    "        #     update_tree_constraints!(gm, bbr)\n",
    "        #     return\n",
    "        # return \n",
    "        # end\n",
    "    else \n",
    "        throw(OCTException(\"Cannot refine $(bbr.name) thresholds without having solved \" *\n",
    "                           \"GlobalModel $(gm.name) with valid approximations first.\" ))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = JuMP.Model(with_optimizer(CPLEX_SILENT))\n",
    "@variable(m, -1 <= x <= 1)\n",
    "@variable(m, y)\n",
    "@constraint(m, 2*x + y >= 0)\n",
    "@objective(m, Min, y)\n",
    "@constraint(m, y >= (x+0.2)^2 - 1)\n",
    "optimize!(m)\n",
    "println(solution(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the feasible set\n",
    "using Plots\n",
    "X = -1:0.01:1\n",
    "f(x) = (x+0.2)^2 - 1\n",
    "g(x) = -2x\n",
    "# the_max = maximum(f(-1:1))\n",
    "\n",
    "plot(X, f, lw = 3)\n",
    "# plot!(X, g, lw = 3)\n",
    "scatter!([0.349], [-0.698], color=:red, size = (600,600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = JuMP.Model(with_optimizer(CPLEX_SILENT))\n",
    "@variable(m, -1 <= x <= 1)\n",
    "@variable(m, y)\n",
    "@constraint(m, 2*x + y >= 0)\n",
    "@objective(m, Min, y)\n",
    "gm = GlobalModel(model = m)\n",
    "add_nonlinear_constraint(gm, :(x -> (x+0.2)^2 - 1), dependent_var = y)\n",
    "bbr = gm.bbls[1]\n",
    "set_param(bbr, :n_samples, 50)\n",
    "uniform_sample_and_eval!(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we did something really naive...\n",
    "learn_constraint!(bbr, threshold = \"upper\" => quantile(bbr.Y, 0.9))\n",
    "update_tree_constraints!(gm, bbr)\n",
    "learn_constraint!(bbr, threshold = \"lower\" => quantile(bbr.Y, 0.01))\n",
    "update_tree_constraints!(gm, bbr)\n",
    "bbr.thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbr.learners[end-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I optimize, and find an underestimator of the real solution \n",
    "optimize!(gm)\n",
    "println(solution(gm), \"  Real y at x: \", bbr(solution(gm))[1])\n",
    "# So I narrow the search, and resample\n",
    "leaf_sample(bbr)\n",
    "refine_thresholds(gm, bbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Progressively tightening thresholds...\")\n",
    "to_print = reshape(collect(values(bbr.thresholds)), (2, Int(length(bbr.thresholds)/2)))\n",
    "println(\"Upper bounds: \", to_print[1,:])\n",
    "println(\"Lower bounds: \", to_print[2,:])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_constraint!(bbr, threshold = \"lower\" => -0.82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbr.learners[end]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
